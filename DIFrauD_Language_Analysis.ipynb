{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFrauD Multilingual Data Quality Assessment\n",
    "## Analyzing Language Diversity Impact on Classification Performance\n",
    "\n",
    "**Course:** COSC 4371 Security Analytics - Fall 2025  \n",
    "**Team Members:** Joseph Mascardo, Niket Gupta  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Objective\n",
    "Investigate whether all samples in the DIFrauD dataset are in English, analyze language distribution by class and domain, and study the impact on classification performance.\n",
    "\n",
    "### Research Questions\n",
    "1. What is the language distribution across classes and domains in DIFrauD?\n",
    "2. How does removing non-English samples affect classifier performance?\n",
    "3. Do transformer-based models handle multilingual content better than traditional ML?\n",
    "\n",
    "---\n",
    "\n",
    "## External Sources and References\n",
    "\n",
    "### Dataset\n",
    "- **DIFrauD Dataset**: https://huggingface.co/datasets/difraud/difraud\n",
    "- **Citation**: Boumber, D., et al. (2024). \"Domain-Agnostic Adapter Architecture for Deception Detection.\" LREC-COLING 2024.\n",
    "\n",
    "### Libraries Used\n",
    "- **langdetect**: https://pypi.org/project/langdetect/ - Language detection (port of Google's language-detection)\n",
    "- **datasets**: https://huggingface.co/docs/datasets/ - HuggingFace datasets library\n",
    "- **transformers**: https://huggingface.co/docs/transformers/ - HuggingFace transformers for DistilBERT\n",
    "- **scikit-learn**: https://scikit-learn.org/ - Traditional ML classifiers\n",
    "- **pandas/numpy**: Data processing\n",
    "- **matplotlib/seaborn**: Visualizations\n",
    "\n",
    "### Key References\n",
    "- Conneau, A., et al. (2020). \"Unsupervised cross-lingual representation learning at scale.\" ACL 2020.\n",
    "- Devlin, J., et al. (2019). \"BERT: Pre-training of deep bidirectional transformers.\" NAACL 2019.\n",
    "- Verma, R. M., et al. (2019). \"Data quality for security challenges.\" ACM CCS 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup and Imports\n",
    "\n",
    "**Steps taken:**\n",
    "1. Install required packages\n",
    "2. Import necessary libraries\n",
    "3. Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (run once)\n# Source: Standard pip installation\n!pip install -q datasets langdetect transformers torch scikit-learn pandas numpy matplotlib seaborn tqdm\n!pip install -q spacy\n!python -m spacy download en_core_web_sm -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data handling\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport time  # For training time tracking\n\n# Dataset loading - Source: https://huggingface.co/docs/datasets/\nfrom datasets import load_dataset, concatenate_datasets\n\n# Language detection - Source: https://pypi.org/project/langdetect/\nfrom langdetect import detect, detect_langs, LangDetectException\n\n# spaCy for language detection validation - Source: https://spacy.io/\nimport spacy\n\n# ML libraries - Source: https://scikit-learn.org/\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, f1_score, \n    precision_score, recall_score, accuracy_score,\n    balanced_accuracy_score\n)\nfrom sklearn.preprocessing import LabelEncoder\n\n# Deep Learning - Source: https://huggingface.co/docs/transformers/\nimport torch\nfrom transformers import (\n    DistilBertTokenizer, DistilBertForSequenceClassification,\n    Trainer, TrainingArguments, EarlyStoppingCallback\n)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Statistical testing\nfrom scipy import stats\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(\"All libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load DIFrauD Dataset\n",
    "\n",
    "**Steps taken:**\n",
    "1. Load all 7 domains from HuggingFace\n",
    "2. Combine train, validation, and test splits\n",
    "3. Create unified DataFrame with domain labels\n",
    "\n",
    "**Dataset Source:** https://huggingface.co/datasets/difraud/difraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define all domains in DIFrauD dataset\nDOMAINS = [\n    'fake_news',\n    'job_scams', \n    'phishing',\n    'political_statements',\n    'product_reviews',\n    'sms',\n    'twitter_rumours'\n]\n\ndef load_difraud_dataset():\n    \"\"\"\n    Load all domains from DIFrauD dataset by directly reading JSONL files.\n    Source: HuggingFace datasets library\n    Dataset: https://huggingface.co/datasets/difraud/difraud\n    \n    Note: The dataset uses legacy loading scripts no longer supported by datasets library,\n    so we load directly from the JSONL files using data_files parameter.\n    \"\"\"\n    all_data = []\n    \n    for domain in tqdm(DOMAINS, desc=\"Loading domains\"):\n        try:\n            # Load directly from JSONL files using data_files parameter\n            # This bypasses the deprecated loading script\n            base_url = f\"https://huggingface.co/datasets/difraud/difraud/resolve/main/{domain}\"\n            \n            dataset = load_dataset(\n                'json',\n                data_files={\n                    'train': f\"{base_url}/train.jsonl\",\n                    'validation': f\"{base_url}/validation.jsonl\",\n                    'test': f\"{base_url}/test.jsonl\"\n                }\n            )\n            \n            # Combine all splits\n            for split in ['train', 'validation', 'test']:\n                if split in dataset:\n                    df_split = dataset[split].to_pandas()\n                    df_split['domain'] = domain\n                    df_split['split'] = split\n                    all_data.append(df_split)\n                    \n        except Exception as e:\n            print(f\"Error loading {domain}: {e}\")\n    \n    # Combine all data\n    if len(all_data) == 0:\n        raise ValueError(\"No data was loaded. Check dataset availability and internet connection.\")\n    \n    df = pd.concat(all_data, ignore_index=True)\n    return df\n\n# Load the dataset\nprint(\"Loading DIFrauD dataset from HuggingFace...\")\nprint(\"(Downloading JSONL files directly - this may take a few minutes)\\n\")\ndf = load_difraud_dataset()\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Total samples: {len(df):,}\")\nprint(f\"Columns: {df.columns.tolist()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- Samples by Domain ---\")\n",
    "domain_counts = df.groupby('domain').agg({\n",
    "    'text': 'count',\n",
    "    'label': ['sum', 'mean']\n",
    "}).round(3)\n",
    "domain_counts.columns = ['Total', 'Deceptive', 'Deceptive_Ratio']\n",
    "domain_counts['Non-Deceptive'] = domain_counts['Total'] - domain_counts['Deceptive']\n",
    "print(domain_counts)\n",
    "\n",
    "print(\"\\n--- Overall Class Distribution ---\")\n",
    "print(f\"Deceptive (label=1): {df['label'].sum():,} ({df['label'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-Deceptive (label=0): {(df['label']==0).sum():,} ({(1-df['label'].mean())*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n--- Sample Text Lengths ---\")\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(df.groupby('domain')['text_length'].describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Language Detection Pipeline\n",
    "\n",
    "**Steps taken:**\n",
    "1. Implement language detection using `langdetect` library\n",
    "2. Handle edge cases (short texts, detection errors)\n",
    "3. Apply to all samples and record detected languages\n",
    "\n",
    "**Source:** langdetect library - https://pypi.org/project/langdetect/  \n",
    "**Note:** langdetect is a port of Google's language-detection library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_safe(text, min_length=20):\n",
    "    \"\"\"\n",
    "    Safely detect language of text with error handling.\n",
    "    \n",
    "    Source: langdetect library (https://pypi.org/project/langdetect/)\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Input text string\n",
    "    - min_length: Minimum text length for reliable detection\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (detected_language_code, confidence_score)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < min_length:\n",
    "        return ('unknown', 0.0)\n",
    "    \n",
    "    try:\n",
    "        # Get language probabilities\n",
    "        langs = detect_langs(text)\n",
    "        # Return top language and its probability\n",
    "        top_lang = langs[0]\n",
    "        return (top_lang.lang, top_lang.prob)\n",
    "    except LangDetectException:\n",
    "        return ('unknown', 0.0)\n",
    "    except Exception as e:\n",
    "        return ('error', 0.0)\n",
    "\n",
    "# Test the function\n",
    "test_texts = [\n",
    "    \"This is a test message in English.\",\n",
    "    \"Ceci est un message de test en français.\",\n",
    "    \"Pathaya enketa maraikara pa\",  # From SMS dataset (Tamil)\n",
    "    \"短文本\"  # Short Chinese text\n",
    "]\n",
    "\n",
    "print(\"Language Detection Test:\")\n",
    "for text in test_texts:\n",
    "    lang, conf = detect_language_safe(text)\n",
    "    print(f\"  '{text[:40]}...' -> {lang} (conf: {conf:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language detection to entire dataset\n",
    "print(\"Detecting languages for all samples...\")\n",
    "print(\"(This may take several minutes)\\n\")\n",
    "\n",
    "# Apply with progress bar\n",
    "tqdm.pandas(desc=\"Detecting languages\")\n",
    "language_results = df['text'].progress_apply(detect_language_safe)\n",
    "\n",
    "# Extract language codes and confidence scores\n",
    "df['detected_language'] = language_results.apply(lambda x: x[0])\n",
    "df['language_confidence'] = language_results.apply(lambda x: x[1])\n",
    "\n",
    "print(\"\\nLanguage detection completed!\")\n",
    "print(f\"Unique languages detected: {df['detected_language'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 spaCy Language Detection and Cross-Validation\n\n**Steps taken:**\n1. Load spaCy English model (en_core_web_sm) for English language detection\n2. Use spaCy's language detection capabilities to validate langdetect results\n3. Cross-validate between langdetect and spaCy detections\n4. Calculate agreement metrics between the two methods\n\n**Source:** spaCy - https://spacy.io/\n\n**Note:** spaCy's en_core_web_sm model is trained on English text, so we use it to validate\nwhether text is likely English based on model processing confidence and token recognition.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load spaCy English model for language validation\n# Source: https://spacy.io/models/en#en_core_web_sm\n\nprint(\"Loading spaCy English model (en_core_web_sm)...\")\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\n    print(\"spaCy model loaded successfully!\")\nexcept OSError:\n    print(\"Downloading en_core_web_sm model...\")\n    import subprocess\n    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n    nlp = spacy.load(\"en_core_web_sm\")\n\ndef detect_english_spacy(text, min_length=20, sample_size=500):\n    \"\"\"\n    Detect if text is English using spaCy's en_core_web_sm model.\n    \n    The approach:\n    1. Process text with English model\n    2. Check ratio of recognized tokens (tokens that are in English vocabulary)\n    3. High ratio of recognized tokens = likely English\n    \n    Source: spaCy documentation - https://spacy.io/\n    \n    Parameters:\n    - text: Input text string\n    - min_length: Minimum text length for reliable detection\n    - sample_size: Max characters to process (for efficiency)\n    \n    Returns:\n    - Tuple of (is_english: bool, confidence: float, details: dict)\n    \"\"\"\n    if not isinstance(text, str) or len(text.strip()) < min_length:\n        return (False, 0.0, {'reason': 'text_too_short'})\n    \n    try:\n        # Sample text for efficiency on long documents\n        text_sample = text[:sample_size] if len(text) > sample_size else text\n        \n        # Process with spaCy\n        doc = nlp(text_sample)\n        \n        # Count tokens and analyze\n        total_tokens = len([t for t in doc if not t.is_space and not t.is_punct])\n        \n        if total_tokens == 0:\n            return (False, 0.0, {'reason': 'no_tokens'})\n        \n        # Count tokens that are recognized (in vocabulary or have vectors)\n        recognized_tokens = sum(1 for t in doc if not t.is_space and not t.is_punct and \n                               (t.is_alpha and (not t.is_oov or t.has_vector)))\n        \n        # Count tokens with English POS tags (meaningful parsing)\n        parsed_tokens = sum(1 for t in doc if not t.is_space and t.pos_ != '')\n        \n        # Count stopwords (English stopwords indicate English text)\n        stopword_count = sum(1 for t in doc if t.is_stop)\n        \n        # Calculate metrics\n        recognition_ratio = recognized_tokens / total_tokens if total_tokens > 0 else 0\n        parse_ratio = parsed_tokens / total_tokens if total_tokens > 0 else 0\n        stopword_ratio = stopword_count / total_tokens if total_tokens > 0 else 0\n        \n        # Composite confidence score\n        # High recognition + parsing + stopwords = likely English\n        confidence = (recognition_ratio * 0.4 + parse_ratio * 0.3 + min(stopword_ratio * 2, 0.3))\n        \n        # Threshold for English detection\n        is_english = confidence > 0.5\n        \n        details = {\n            'total_tokens': total_tokens,\n            'recognized_tokens': recognized_tokens,\n            'recognition_ratio': round(recognition_ratio, 3),\n            'stopword_ratio': round(stopword_ratio, 3),\n            'parse_ratio': round(parse_ratio, 3)\n        }\n        \n        return (is_english, round(confidence, 3), details)\n        \n    except Exception as e:\n        return (False, 0.0, {'reason': f'error: {str(e)}'})\n\n# Test spaCy detection\nprint(\"\\nspaCy English Detection Test:\")\nfor text in test_texts:\n    is_eng, conf, details = detect_english_spacy(text)\n    print(f\"  '{text[:40]}...' -> English={is_eng} (conf: {conf:.2f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Apply spaCy English detection to the dataset and cross-validate with langdetect\n# Note: Processing entire dataset with spaCy is slow, so we use sampling for validation\n\nprint(\"=\"*60)\nprint(\"CROSS-VALIDATION: langdetect vs spaCy\")\nprint(\"=\"*60)\n\n# Sample for cross-validation (processing full dataset with spaCy is computationally expensive)\nCROSS_VAL_SAMPLE_SIZE = 2000\nnp.random.seed(SEED)\nsample_indices = np.random.choice(len(df), min(CROSS_VAL_SAMPLE_SIZE, len(df)), replace=False)\ndf_sample = df.iloc[sample_indices].copy()\n\nprint(f\"\\nCross-validating on {len(df_sample)} samples...\")\n\n# Apply spaCy detection to sample\ntqdm.pandas(desc=\"spaCy detection\")\nspacy_results = df_sample['text'].progress_apply(detect_english_spacy)\n\ndf_sample['spacy_is_english'] = spacy_results.apply(lambda x: x[0])\ndf_sample['spacy_confidence'] = spacy_results.apply(lambda x: x[1])\n\n# Compare results\n# langdetect says English (is_english == True) vs spaCy says English (spacy_is_english == True)\nagreement = (df_sample['is_english'] == df_sample['spacy_is_english']).mean()\nprint(f\"\\n--- Agreement Metrics ---\")\nprint(f\"Overall agreement rate: {agreement*100:.2f}%\")\n\n# Confusion matrix between the two methods\ncross_tab = pd.crosstab(\n    df_sample['is_english'].map({True: 'langdetect: English', False: 'langdetect: Non-English'}),\n    df_sample['spacy_is_english'].map({True: 'spaCy: English', False: 'spaCy: Non-English'})\n)\nprint(\"\\nCross-tabulation (langdetect vs spaCy):\")\nprint(cross_tab)\n\n# Calculate Cohen's Kappa for inter-rater agreement\nfrom sklearn.metrics import cohen_kappa_score\nkappa = cohen_kappa_score(df_sample['is_english'], df_sample['spacy_is_english'])\nprint(f\"\\nCohen's Kappa (inter-method agreement): {kappa:.4f}\")\nprint(f\"  Interpretation: \", end=\"\")\nif kappa < 0.20:\n    print(\"Poor agreement\")\nelif kappa < 0.40:\n    print(\"Fair agreement\")\nelif kappa < 0.60:\n    print(\"Moderate agreement\")\nelif kappa < 0.80:\n    print(\"Substantial agreement\")\nelse:\n    print(\"Almost perfect agreement\")\n\n# Analyze disagreements\ndisagreements = df_sample[df_sample['is_english'] != df_sample['spacy_is_english']]\nprint(f\"\\n--- Disagreement Analysis ---\")\nprint(f\"Total disagreements: {len(disagreements)} ({len(disagreements)/len(df_sample)*100:.2f}%)\")\n\nif len(disagreements) > 0:\n    # langdetect says English, spaCy says Non-English\n    ld_eng_sp_non = disagreements[(disagreements['is_english'] == True) & (disagreements['spacy_is_english'] == False)]\n    print(f\"  langdetect=English, spaCy=Non-English: {len(ld_eng_sp_non)}\")\n    \n    # langdetect says Non-English, spaCy says English\n    ld_non_sp_eng = disagreements[(disagreements['is_english'] == False) & (disagreements['spacy_is_english'] == True)]\n    print(f\"  langdetect=Non-English, spaCy=English: {len(ld_non_sp_eng)}\")\n    \n    # Show examples of disagreements\n    print(\"\\nExamples of disagreements:\")\n    for i, (idx, row) in enumerate(disagreements.head(3).iterrows()):\n        print(f\"\\n  Example {i+1} (Domain: {row['domain']}):\")\n        print(f\"    Text: '{row['text'][:100]}...'\")\n        print(f\"    langdetect: {row['detected_language']} (conf: {row['language_confidence']:.2f})\")\n        print(f\"    spaCy English: {row['spacy_is_english']} (conf: {row['spacy_confidence']:.2f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Manual Validation of Language Detection\n\n**Purpose:** Document the manual validation process for language detection results.\n\nThis section provides a framework for manually validating a sample of language detection results \nto assess the accuracy of automated detection methods.\n\n**Methodology:**\n1. Randomly sample 100-500 instances from the dataset\n2. Manually review each sample's detected language\n3. Record agreement/disagreement with automated detection\n4. Calculate validation metrics (accuracy, error types)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Manual Validation Framework for Language Detection\n# This cell generates a sample for manual validation and documents the process\n\nprint(\"=\"*80)\nprint(\"MANUAL VALIDATION SAMPLE\")\nprint(\"=\"*80)\n\n# Generate stratified sample for manual validation\nMANUAL_VAL_SIZE = 200  # Sample size (100-500 recommended)\nnp.random.seed(SEED)\n\n# Stratified sampling: ensure representation from each domain and both language classes\nmanual_val_samples = []\n\nfor domain in DOMAINS:\n    domain_data = df[df['domain'] == domain]\n    \n    # Sample from English and Non-English\n    english_samples = domain_data[domain_data['is_english'] == True]\n    non_english_samples = domain_data[domain_data['is_english'] == False]\n    \n    # Take proportional samples\n    n_per_domain = MANUAL_VAL_SIZE // len(DOMAINS)\n    n_english = min(len(english_samples), n_per_domain // 2)\n    n_non_english = min(len(non_english_samples), n_per_domain // 2)\n    \n    if n_english > 0:\n        manual_val_samples.append(english_samples.sample(n=n_english, random_state=SEED))\n    if n_non_english > 0:\n        manual_val_samples.append(non_english_samples.sample(n=n_non_english, random_state=SEED))\n\nmanual_val_df = pd.concat(manual_val_samples, ignore_index=True)\nprint(f\"Manual validation sample size: {len(manual_val_df)}\")\nprint(f\"Domains represented: {manual_val_df['domain'].nunique()}\")\n\n# Display sample distribution\nprint(\"\\n--- Sample Distribution ---\")\nprint(manual_val_df.groupby(['domain', 'is_english']).size().unstack(fill_value=0))\n\n# Generate validation template\nprint(\"\\n--- Manual Validation Template ---\")\nprint(\"For each sample below, verify if the detected language is correct.\")\nprint(\"Record your assessment as: CORRECT, INCORRECT, or UNCERTAIN\\n\")\n\n# Show sample entries for manual review\nprint(\"Sample entries for manual validation:\")\nprint(\"-\" * 80)\n\nfor i, (idx, row) in enumerate(manual_val_df.head(10).iterrows()):\n    print(f\"\\n[Sample {i+1}] Domain: {row['domain']}\")\n    print(f\"  Text: '{row['text'][:150]}...'\")\n    print(f\"  Detected: {row['detected_language']} (confidence: {row['language_confidence']:.2f})\")\n    print(f\"  Auto-classified as English: {row['is_english']}\")\n    print(f\"  Manual Assessment: _________ (CORRECT / INCORRECT / UNCERTAIN)\")\n\n# Save validation sample to CSV for offline manual review\nmanual_val_df[['text', 'label', 'domain', 'detected_language', 'language_confidence', 'is_english']].to_csv(\n    'manual_validation_sample.csv', index=False\n)\nprint(\"\\n\" + \"-\" * 80)\nprint(f\"\\nFull validation sample saved to 'manual_validation_sample.csv'\")\nprint(\"Use this file for systematic manual validation of language detection results.\")\n\n# Document validation process\nprint(\"\\n\" + \"=\"*80)\nprint(\"MANUAL VALIDATION DOCUMENTATION\")\nprint(\"=\"*80)\nprint(\"\"\"\nVALIDATION PROCESS:\n1. Sample Selection: Stratified sample of {size} instances across all domains\n2. Review Criteria:\n   - CORRECT: Detected language matches actual language of text\n   - INCORRECT: Detected language does not match actual language\n   - UNCERTAIN: Text is ambiguous, code-mixed, or too short to determine\n\nEXPECTED OUTCOMES:\n- Calculate agreement rate between automated and manual detection\n- Identify systematic errors (e.g., specific languages misclassified)\n- Estimate precision and recall of English detection\n\nVALIDATION METRICS TO CALCULATE:\n- Manual agreement rate = (CORRECT assessments) / (total assessments)\n- Error analysis: distribution of INCORRECT by domain and detected language\n- Confidence calibration: compare detection confidence vs accuracy\n\nNOTE: After completing manual validation, update this section with results.\n\"\"\".format(size=MANUAL_VAL_SIZE))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Language Distribution Analysis\n",
    "\n",
    "**Steps taken:**\n",
    "1. Calculate language distribution overall\n",
    "2. Analyze by class (deceptive vs non-deceptive)\n",
    "3. Analyze by domain\n",
    "4. Perform chi-square tests for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall language distribution\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL LANGUAGE DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lang_counts = df['detected_language'].value_counts()\n",
    "lang_percentages = df['detected_language'].value_counts(normalize=True) * 100\n",
    "\n",
    "lang_summary = pd.DataFrame({\n",
    "    'Count': lang_counts,\n",
    "    'Percentage': lang_percentages.round(2)\n",
    "})\n",
    "print(lang_summary.head(15))\n",
    "\n",
    "# English vs Non-English\n",
    "df['is_english'] = df['detected_language'] == 'en'\n",
    "print(f\"\\n--- English vs Non-English ---\")\n",
    "print(f\"English samples: {df['is_english'].sum():,} ({df['is_english'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-English samples: {(~df['is_english']).sum():,} ({(~df['is_english']).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution by CLASS (deceptive vs non-deceptive)\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_lang_dist = pd.crosstab(\n",
    "    df['label'].map({0: 'Non-Deceptive', 1: 'Deceptive'}),\n",
    "    df['is_english'].map({True: 'English', False: 'Non-English'}),\n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nCounts:\")\n",
    "print(class_lang_dist)\n",
    "\n",
    "# Percentages within each class\n",
    "class_lang_pct = pd.crosstab(\n",
    "    df['label'].map({0: 'Non-Deceptive', 1: 'Deceptive'}),\n",
    "    df['is_english'].map({True: 'English', False: 'Non-English'}),\n",
    "    normalize='index'\n",
    ") * 100\n",
    "print(\"\\nPercentages (within each class):\")\n",
    "print(class_lang_pct.round(2))\n",
    "\n",
    "# Chi-square test for class vs language\n",
    "contingency = pd.crosstab(df['label'], df['is_english'])\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "print(f\"\\nChi-square test (Class vs Language):\")\n",
    "print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4e}\")\n",
    "print(f\"  Significant (p < 0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution by DOMAIN\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY DOMAIN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "domain_lang_analysis = []\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    \n",
    "    total = len(domain_df)\n",
    "    english = domain_df['is_english'].sum()\n",
    "    non_english = total - english\n",
    "    \n",
    "    # Top non-English languages\n",
    "    non_eng_langs = domain_df[~domain_df['is_english']]['detected_language'].value_counts().head(3)\n",
    "    top_non_eng = ', '.join([f\"{lang}({cnt})\" for lang, cnt in non_eng_langs.items()])\n",
    "    \n",
    "    domain_lang_analysis.append({\n",
    "        'Domain': domain,\n",
    "        'Total': total,\n",
    "        'English': english,\n",
    "        'Non-English': non_english,\n",
    "        'English %': (english/total*100),\n",
    "        'Non-English %': (non_english/total*100),\n",
    "        'Top Non-English Languages': top_non_eng\n",
    "    })\n",
    "\n",
    "domain_lang_df = pd.DataFrame(domain_lang_analysis)\n",
    "print(domain_lang_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown: Language distribution by Domain AND Class\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY DOMAIN AND CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "detailed_analysis = []\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    for label in [0, 1]:\n",
    "        subset = df[(df['domain'] == domain) & (df['label'] == label)]\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "            \n",
    "        total = len(subset)\n",
    "        english = subset['is_english'].sum()\n",
    "        \n",
    "        # Get top 5 detected languages\n",
    "        lang_dist = subset['detected_language'].value_counts().head(5).to_dict()\n",
    "        \n",
    "        detailed_analysis.append({\n",
    "            'Domain': domain,\n",
    "            'Class': 'Deceptive' if label == 1 else 'Non-Deceptive',\n",
    "            'Total': total,\n",
    "            'English': english,\n",
    "            'English %': round(english/total*100, 2),\n",
    "            'Non-English': total - english,\n",
    "            'Non-English %': round((total-english)/total*100, 2),\n",
    "            'Languages': lang_dist\n",
    "        })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_analysis)\n",
    "print(detailed_df[['Domain', 'Class', 'Total', 'English', 'English %', 'Non-English', 'Non-English %']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Overall language distribution (top 10)\n",
    "ax1 = axes[0, 0]\n",
    "top_langs = df['detected_language'].value_counts().head(10)\n",
    "colors = ['green' if lang == 'en' else 'coral' for lang in top_langs.index]\n",
    "top_langs.plot(kind='bar', ax=ax1, color=colors)\n",
    "ax1.set_title('Top 10 Detected Languages', fontsize=12)\n",
    "ax1.set_xlabel('Language Code')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: English vs Non-English by domain\n",
    "ax2 = axes[0, 1]\n",
    "domain_lang_pivot = df.groupby('domain')['is_english'].agg(['sum', 'count'])\n",
    "domain_lang_pivot['non_english'] = domain_lang_pivot['count'] - domain_lang_pivot['sum']\n",
    "domain_lang_pivot[['sum', 'non_english']].plot(kind='bar', stacked=True, ax=ax2, \n",
    "                                                color=['green', 'coral'])\n",
    "ax2.set_title('English vs Non-English by Domain', fontsize=12)\n",
    "ax2.set_xlabel('Domain')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.legend(['English', 'Non-English'])\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Non-English percentage by domain\n",
    "ax3 = axes[1, 0]\n",
    "non_eng_pct = domain_lang_df.set_index('Domain')['Non-English %']\n",
    "non_eng_pct.plot(kind='bar', ax=ax3, color='coral')\n",
    "ax3.set_title('Non-English Percentage by Domain', fontsize=12)\n",
    "ax3.set_xlabel('Domain')\n",
    "ax3.set_ylabel('Non-English %')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.axhline(y=non_eng_pct.mean(), color='red', linestyle='--', label=f'Mean: {non_eng_pct.mean():.1f}%')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Language distribution by class\n",
    "ax4 = axes[1, 1]\n",
    "class_lang_pct.plot(kind='bar', ax=ax4, color=['green', 'coral'])\n",
    "ax4.set_title('Language Distribution by Class', fontsize=12)\n",
    "ax4.set_xlabel('Class')\n",
    "ax4.set_ylabel('Percentage')\n",
    "ax4.legend(['English', 'Non-English'])\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('language_distribution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'language_distribution_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create Dataset Splits (English-only vs Full)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Create filtered English-only dataset\n",
    "2. Create full multilingual dataset\n",
    "3. Ensure consistent train/test splits for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create English-only and Full datasets\n",
    "print(\"Creating dataset versions...\\n\")\n",
    "\n",
    "# Full dataset (all languages)\n",
    "df_full = df.copy()\n",
    "\n",
    "# English-only dataset\n",
    "df_english = df[df['is_english'] == True].copy()\n",
    "\n",
    "print(f\"Full dataset: {len(df_full):,} samples\")\n",
    "print(f\"English-only dataset: {len(df_english):,} samples\")\n",
    "print(f\"Samples removed: {len(df_full) - len(df_english):,} ({(1 - len(df_english)/len(df_full))*100:.2f}%)\")\n",
    "\n",
    "# Compare class distribution\n",
    "print(\"\\n--- Class Distribution Comparison ---\")\n",
    "print(f\"Full - Deceptive: {df_full['label'].mean()*100:.2f}%\")\n",
    "print(f\"English-only - Deceptive: {df_english['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare stratified train/test splits.\n",
    "    Uses stratification to handle class imbalance.\n",
    "    \n",
    "    Source: scikit-learn train_test_split\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    \"\"\"\n",
    "    X = df['text'].values\n",
    "    y = df['label'].values\n",
    "    domains = df['domain'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, domains_train, domains_test = train_test_split(\n",
    "        X, y, domains,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, domains_train, domains_test\n",
    "\n",
    "# Prepare data for both versions\n",
    "print(\"Preparing train/test splits...\\n\")\n",
    "\n",
    "# Full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full, domains_train_full, domains_test_full = \\\n",
    "    prepare_train_test_data(df_full)\n",
    "\n",
    "# English-only dataset\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng, domains_train_eng, domains_test_eng = \\\n",
    "    prepare_train_test_data(df_english)\n",
    "\n",
    "print(\"Full Dataset:\")\n",
    "print(f\"  Train: {len(X_train_full):,} | Test: {len(X_test_full):,}\")\n",
    "print(f\"  Train class dist: {np.mean(y_train_full)*100:.2f}% deceptive\")\n",
    "\n",
    "print(\"\\nEnglish-only Dataset:\")\n",
    "print(f\"  Train: {len(X_train_eng):,} | Test: {len(X_test_eng):,}\")\n",
    "print(f\"  Train class dist: {np.mean(y_train_eng)*100:.2f}% deceptive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Traditional ML Classifiers (Random Forest & SVM)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Create TF-IDF features\n",
    "2. Train Random Forest and SVM classifiers\n",
    "3. Evaluate on both dataset versions\n",
    "4. Use F1-score as primary metric (suitable for imbalanced data)\n",
    "\n",
    "**Source:** scikit-learn - https://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(X_train, X_test, max_features=10000):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features from text data.\n",
    "    \n",
    "    Source: scikit-learn TfidfVectorizer\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "        min_df=2,           # Minimum document frequency\n",
    "        max_df=0.95,        # Maximum document frequency\n",
    "        sublinear_tf=True   # Apply sublinear tf scaling\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, vectorizer\n",
    "\n",
    "print(\"Creating TF-IDF features...\\n\")\n",
    "\n",
    "# Full dataset features\n",
    "X_train_full_tfidf, X_test_full_tfidf, vectorizer_full = \\\n",
    "    create_tfidf_features(X_train_full, X_test_full)\n",
    "print(f\"Full dataset - TF-IDF shape: {X_train_full_tfidf.shape}\")\n",
    "\n",
    "# English-only features\n",
    "X_train_eng_tfidf, X_test_eng_tfidf, vectorizer_eng = \\\n",
    "    create_tfidf_features(X_train_eng, X_test_eng)\n",
    "print(f\"English-only - TF-IDF shape: {X_train_eng_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(clf, X_train, X_test, y_train, y_test, clf_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Train classifier and return evaluation metrics.\n",
    "    \n",
    "    Uses metrics suitable for imbalanced datasets:\n",
    "    - F1-Score (weighted and macro)\n",
    "    - Balanced Accuracy\n",
    "    - Precision and Recall\n",
    "    \n",
    "    Source: scikit-learn metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {clf_name} on {dataset_name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Classifier': clf_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Balanced_Accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'F1_Weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'F1_Macro': f1_score(y_test, y_pred, average='macro'),\n",
    "        'Precision_Weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall_Weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    print(f\"  F1 (weighted): {metrics['F1_Weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro): {metrics['F1_Macro']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {metrics['Balanced_Accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Random Forest with timing\n# Source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nprint(\"=\"*60)\nprint(\"RANDOM FOREST CLASSIFIER\")\nprint(\"=\"*60)\n\nrf_results = []\ntraining_times = {}  # Track training times for all models\n\n# Random Forest on Full Dataset\nrf_full = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,\n    min_samples_split=2,\n    class_weight='balanced',  # Handle class imbalance\n    random_state=SEED,\n    n_jobs=-1\n)\n\nstart_time = time.time()\nmetrics_rf_full, pred_rf_full, _ = train_and_evaluate_classifier(\n    rf_full, X_train_full_tfidf, X_test_full_tfidf,\n    y_train_full, y_test_full,\n    'Random Forest', 'Full (Multilingual)'\n)\ntraining_times['RF_Full'] = time.time() - start_time\nprint(f\"  Training time: {training_times['RF_Full']:.2f} seconds\")\nrf_results.append(metrics_rf_full)\n\n# Random Forest on English-only Dataset\nrf_eng = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=None,\n    min_samples_split=2,\n    class_weight='balanced',\n    random_state=SEED,\n    n_jobs=-1\n)\n\nstart_time = time.time()\nmetrics_rf_eng, pred_rf_eng, _ = train_and_evaluate_classifier(\n    rf_eng, X_train_eng_tfidf, X_test_eng_tfidf,\n    y_train_eng, y_test_eng,\n    'Random Forest', 'English-only'\n)\ntraining_times['RF_English'] = time.time() - start_time\nprint(f\"  Training time: {training_times['RF_English']:.2f} seconds\")\nrf_results.append(metrics_rf_eng)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train SVM (Support Vector Machine) using LinearSVC with timing\n# Source: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n# Note: LinearSVC is faster than SVC with kernel='linear' for large sparse datasets like TF-IDF\n\nprint(\"=\"*60)\nprint(\"SVM CLASSIFIER (LinearSVC)\")\nprint(\"=\"*60)\n\nsvm_results = []\n\n# LinearSVC on Full Dataset\nsvm_full = LinearSVC(\n    C=1.0,\n    class_weight='balanced',\n    max_iter=10000,\n    random_state=SEED\n)\n\nstart_time = time.time()\nmetrics_svm_full, pred_svm_full, _ = train_and_evaluate_classifier(\n    svm_full, X_train_full_tfidf, X_test_full_tfidf,\n    y_train_full, y_test_full,\n    'SVM (LinearSVC)', 'Full (Multilingual)'\n)\ntraining_times['SVM_Full'] = time.time() - start_time\nprint(f\"  Training time: {training_times['SVM_Full']:.2f} seconds\")\nsvm_results.append(metrics_svm_full)\n\n# LinearSVC on English-only Dataset\nsvm_eng = LinearSVC(\n    C=1.0,\n    class_weight='balanced',\n    max_iter=10000,\n    random_state=SEED\n)\n\nstart_time = time.time()\nmetrics_svm_eng, pred_svm_eng, _ = train_and_evaluate_classifier(\n    svm_eng, X_train_eng_tfidf, X_test_eng_tfidf,\n    y_train_eng, y_test_eng,\n    'SVM (LinearSVC)', 'English-only'\n)\ntraining_times['SVM_English'] = time.time() - start_time\nprint(f\"  Training time: {training_times['SVM_English']:.2f} seconds\")\nsvm_results.append(metrics_svm_eng)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Transformer-Based Classifier (DistilBERT)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Load pretrained DistilBERT model and tokenizer\n",
    "2. Fine-tune on both dataset versions\n",
    "3. Evaluate performance\n",
    "\n",
    "**Source:** HuggingFace Transformers - https://huggingface.co/docs/transformers/  \n",
    "**Model:** distilbert-base-uncased - https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT Dataset Class\n",
    "class FraudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for fraud detection.\n",
    "    Source: PyTorch Dataset API\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for HuggingFace Trainer.\n",
    "    Uses metrics suitable for imbalanced data.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'balanced_accuracy': balanced_accuracy_score(labels, predictions),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'precision': precision_score(labels, predictions, average='weighted'),\n",
    "        'recall': recall_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_distilbert(X_train, X_test, y_train, y_test, dataset_name, epochs=3, batch_size=16):\n    \"\"\"\n    Train DistilBERT classifier with training time tracking.\n    \n    Source: HuggingFace Transformers\n    Model: distilbert-base-uncased\n    https://huggingface.co/distilbert-base-uncased\n    \n    Returns:\n    - metrics: dict of evaluation metrics\n    - y_pred: predictions on test set\n    - model: trained model\n    - train_time: training time in seconds\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Training DistilBERT on {dataset_name}\")\n    print(f\"{'='*60}\")\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Load tokenizer and model\n    model_name = 'distilbert-base-uncased'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    model = DistilBertForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=2\n    )\n    \n    # Create datasets\n    train_dataset = FraudDataset(X_train, y_train, tokenizer)\n    test_dataset = FraudDataset(X_test, y_test, tokenizer)\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=f'./results_{dataset_name.replace(\" \", \"_\")}',\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=100,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='f1_weighted',\n        seed=SEED\n    )\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n    )\n    \n    # Train\n    trainer.train()\n    \n    # Calculate training time\n    train_time = time.time() - start_time\n    \n    # Evaluate\n    eval_results = trainer.evaluate()\n    \n    # Get predictions for detailed metrics\n    predictions = trainer.predict(test_dataset)\n    y_pred = np.argmax(predictions.predictions, axis=1)\n    \n    metrics = {\n        'Classifier': 'DistilBERT',\n        'Dataset': dataset_name,\n        'Accuracy': eval_results['eval_accuracy'],\n        'Balanced_Accuracy': eval_results['eval_balanced_accuracy'],\n        'F1_Weighted': eval_results['eval_f1_weighted'],\n        'F1_Macro': eval_results['eval_f1_macro'],\n        'Precision_Weighted': eval_results['eval_precision'],\n        'Recall_Weighted': eval_results['eval_recall']\n    }\n    \n    print(f\"\\nResults for {dataset_name}:\")\n    print(f\"  F1 (weighted): {metrics['F1_Weighted']:.4f}\")\n    print(f\"  F1 (macro): {metrics['F1_Macro']:.4f}\")\n    print(f\"  Balanced Accuracy: {metrics['Balanced_Accuracy']:.4f}\")\n    print(f\"  Training time: {train_time:.2f} seconds ({train_time/60:.2f} minutes)\")\n    \n    return metrics, y_pred, model, train_time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DistilBERT on both datasets\n",
    "# Note: This may take significant time depending on GPU availability\n",
    "\n",
    "distilbert_results = []\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Sample size for faster training (optional - remove for full training)\n",
    "# Comment out these lines for full dataset training\n",
    "SAMPLE_SIZE = 5000  # Use smaller sample for demonstration\n",
    "print(f\"\\nNote: Using sample of {SAMPLE_SIZE} for demonstration.\")\n",
    "print(\"Remove SAMPLE_SIZE limit for full training.\\n\")\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(SEED)\n",
    "sample_idx_full = np.random.choice(len(X_train_full), min(SAMPLE_SIZE, len(X_train_full)), replace=False)\n",
    "sample_idx_eng = np.random.choice(len(X_train_eng), min(SAMPLE_SIZE, len(X_train_eng)), replace=False)\n",
    "\n",
    "X_train_full_sample = X_train_full[sample_idx_full]\n",
    "y_train_full_sample = y_train_full[sample_idx_full]\n",
    "\n",
    "X_train_eng_sample = X_train_eng[sample_idx_eng]\n",
    "y_train_eng_sample = y_train_eng[sample_idx_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train on Full Dataset\nmetrics_bert_full, pred_bert_full, model_full, bert_time_full = train_distilbert(\n    X_train_full_sample, X_test_full[:1000],  # Smaller test set for speed\n    y_train_full_sample, y_test_full[:1000],\n    'Full (Multilingual)',\n    epochs=2,\n    batch_size=16\n)\ntraining_times['DistilBERT_Full'] = bert_time_full\ndistilbert_results.append(metrics_bert_full)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train on English-only Dataset\nmetrics_bert_eng, pred_bert_eng, model_eng, bert_time_eng = train_distilbert(\n    X_train_eng_sample, X_test_eng[:1000],\n    y_train_eng_sample, y_test_eng[:1000],\n    'English-only',\n    epochs=2,\n    batch_size=16\n)\ntraining_times['DistilBERT_English'] = bert_time_eng\ndistilbert_results.append(metrics_bert_eng)\n\n# Print training time summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING TIME SUMMARY\")\nprint(\"=\"*60)\nfor model_name, train_time in training_times.items():\n    print(f\"  {model_name}: {train_time:.2f}s ({train_time/60:.2f}m)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Results Comparison and Analysis\n",
    "\n",
    "**Steps taken:**\n",
    "1. Compile all results\n",
    "2. Calculate domain-wise performance\n",
    "3. Compute aggregate metrics (mean and weighted)\n",
    "4. Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = rf_results + svm_results + distilbert_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance difference\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE DIFFERENCE (English-only vs Full)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for classifier in ['Random Forest', 'SVM', 'DistilBERT']:\n",
    "    clf_results = results_df[results_df['Classifier'] == classifier]\n",
    "    \n",
    "    if len(clf_results) < 2:\n",
    "        continue\n",
    "        \n",
    "    full_f1 = clf_results[clf_results['Dataset'].str.contains('Full')]['F1_Weighted'].values[0]\n",
    "    eng_f1 = clf_results[clf_results['Dataset'].str.contains('English')]['F1_Weighted'].values[0]\n",
    "    \n",
    "    diff = eng_f1 - full_f1\n",
    "    pct_change = (diff / full_f1) * 100\n",
    "    \n",
    "    print(f\"\\n{classifier}:\")\n",
    "    print(f\"  Full dataset F1: {full_f1:.4f}\")\n",
    "    print(f\"  English-only F1: {eng_f1:.4f}\")\n",
    "    print(f\"  Difference: {diff:+.4f} ({pct_change:+.2f}%)\")\n",
    "    print(f\"  Impact: {'Improved' if diff > 0 else 'Decreased'} with English-only data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 8.1 Statistical Significance Testing\n\n**Steps taken:**\n1. Calculate Cohen's d effect size for all model comparisons\n2. Perform paired t-tests comparing English-only vs Full dataset performance\n3. Use cross-validation to obtain multiple performance measurements for statistical testing\n\n**Source:** \n- Cohen's d: Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences\n- scipy.stats.ttest_rel: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cohen's d Effect Size and Paired t-tests\n# Source: Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences\n\ndef cohens_d(group1, group2):\n    \"\"\"\n    Calculate Cohen's d effect size for two groups.\n    \n    Cohen's d = (mean1 - mean2) / pooled_std\n    \n    Interpretation (Cohen, 1988):\n    - |d| < 0.2: negligible effect\n    - 0.2 <= |d| < 0.5: small effect\n    - 0.5 <= |d| < 0.8: medium effect\n    - |d| >= 0.8: large effect\n    \n    Source: Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences\n    \"\"\"\n    n1, n2 = len(group1), len(group2)\n    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n    \n    # Pooled standard deviation\n    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n    \n    if pooled_std == 0:\n        return 0.0\n    \n    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    return d\n\ndef interpret_cohens_d(d):\n    \"\"\"Interpret Cohen's d effect size.\"\"\"\n    abs_d = abs(d)\n    if abs_d < 0.2:\n        return \"negligible\"\n    elif abs_d < 0.5:\n        return \"small\"\n    elif abs_d < 0.8:\n        return \"medium\"\n    else:\n        return \"large\"\n\nprint(\"=\"*80)\nprint(\"COHEN'S D EFFECT SIZE ANALYSIS\")\nprint(\"=\"*80)\nprint(\"\\nComparing English-only vs Full dataset performance across classifiers\")\nprint(\"(Positive d = English-only performs better; Negative d = Full performs better)\\n\")\n\n# We need cross-validation scores for proper paired t-tests\n# Let's run stratified k-fold CV to get multiple measurements\n\nprint(\"Running 5-fold cross-validation for statistical testing...\")\nprint(\"(This provides multiple performance measurements for paired t-tests)\\n\")\n\nfrom sklearn.model_selection import StratifiedKFold\n\nN_FOLDS = 5\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# Store CV results for statistical analysis\ncv_results = {\n    'RF_Full': [], 'RF_English': [],\n    'SVM_Full': [], 'SVM_English': []\n}\n\n# Cross-validation for Random Forest\nprint(\"Cross-validating Random Forest...\")\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full_tfidf, y_train_full)):\n    # Full dataset\n    rf_cv = RandomForestClassifier(n_estimators=100, class_weight='balanced', \n                                    random_state=SEED, n_jobs=-1)\n    rf_cv.fit(X_train_full_tfidf[train_idx], y_train_full[train_idx])\n    y_pred_cv = rf_cv.predict(X_train_full_tfidf[val_idx])\n    cv_results['RF_Full'].append(f1_score(y_train_full[val_idx], y_pred_cv, average='weighted'))\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X_train_eng_tfidf, y_train_eng)):\n    # English-only dataset\n    rf_cv = RandomForestClassifier(n_estimators=100, class_weight='balanced', \n                                    random_state=SEED, n_jobs=-1)\n    rf_cv.fit(X_train_eng_tfidf[train_idx], y_train_eng[train_idx])\n    y_pred_cv = rf_cv.predict(X_train_eng_tfidf[val_idx])\n    cv_results['RF_English'].append(f1_score(y_train_eng[val_idx], y_pred_cv, average='weighted'))\n\n# Cross-validation for SVM\nprint(\"Cross-validating SVM...\")\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full_tfidf, y_train_full)):\n    svm_cv = LinearSVC(C=1.0, class_weight='balanced', max_iter=10000, random_state=SEED)\n    svm_cv.fit(X_train_full_tfidf[train_idx], y_train_full[train_idx])\n    y_pred_cv = svm_cv.predict(X_train_full_tfidf[val_idx])\n    cv_results['SVM_Full'].append(f1_score(y_train_full[val_idx], y_pred_cv, average='weighted'))\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X_train_eng_tfidf, y_train_eng)):\n    svm_cv = LinearSVC(C=1.0, class_weight='balanced', max_iter=10000, random_state=SEED)\n    svm_cv.fit(X_train_eng_tfidf[train_idx], y_train_eng[train_idx])\n    y_pred_cv = svm_cv.predict(X_train_eng_tfidf[val_idx])\n    cv_results['SVM_English'].append(f1_score(y_train_eng[val_idx], y_pred_cv, average='weighted'))\n\nprint(\"\\nCross-validation complete!\\n\")\n\n# Calculate Cohen's d for each classifier\neffect_sizes = {}\n\nprint(\"--- Cohen's d Effect Sizes ---\")\nfor clf_name in ['RF', 'SVM']:\n    full_scores = np.array(cv_results[f'{clf_name}_Full'])\n    eng_scores = np.array(cv_results[f'{clf_name}_English'])\n    \n    d = cohens_d(eng_scores, full_scores)\n    effect_sizes[clf_name] = d\n    interpretation = interpret_cohens_d(d)\n    \n    print(f\"\\n{clf_name}:\")\n    print(f\"  Full dataset CV F1: {full_scores.mean():.4f} (+/- {full_scores.std():.4f})\")\n    print(f\"  English-only CV F1: {eng_scores.mean():.4f} (+/- {eng_scores.std():.4f})\")\n    print(f\"  Cohen's d: {d:.4f} ({interpretation} effect)\")\n    print(f\"  Direction: {'English-only better' if d > 0 else 'Full better'}\")\n\n# Paired t-tests\nprint(\"\\n\" + \"=\"*80)\nprint(\"PAIRED T-TESTS (English-only vs Full)\")\nprint(\"=\"*80)\nprint(\"\\nUsing scipy.stats.ttest_rel for paired comparisons\")\nprint(\"H0: No difference in performance between English-only and Full datasets\")\nprint(\"H1: There is a significant difference in performance\\n\")\n\nttest_results = {}\n\nfor clf_name in ['RF', 'SVM']:\n    full_scores = np.array(cv_results[f'{clf_name}_Full'])\n    eng_scores = np.array(cv_results[f'{clf_name}_English'])\n    \n    # Paired t-test\n    # Source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n    t_stat, p_value = stats.ttest_rel(eng_scores, full_scores)\n    \n    ttest_results[clf_name] = {'t_stat': t_stat, 'p_value': p_value}\n    \n    print(f\"{clf_name}:\")\n    print(f\"  t-statistic: {t_stat:.4f}\")\n    print(f\"  p-value: {p_value:.4f}\")\n    print(f\"  Significant at alpha=0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n    print(f\"  Significant at alpha=0.01: {'Yes' if p_value < 0.01 else 'No'}\")\n    print()\n\n# Summary table\nprint(\"\\n--- Statistical Analysis Summary ---\")\nsummary_data = []\nfor clf_name in ['RF', 'SVM']:\n    full_scores = np.array(cv_results[f'{clf_name}_Full'])\n    eng_scores = np.array(cv_results[f'{clf_name}_English'])\n    d = effect_sizes[clf_name]\n    t_res = ttest_results[clf_name]\n    \n    summary_data.append({\n        'Classifier': 'Random Forest' if clf_name == 'RF' else 'SVM (LinearSVC)',\n        'Full_F1_Mean': f\"{full_scores.mean():.4f}\",\n        'English_F1_Mean': f\"{eng_scores.mean():.4f}\",\n        'Cohens_d': f\"{d:.4f}\",\n        'Effect_Size': interpret_cohens_d(d),\n        't_stat': f\"{t_res['t_stat']:.4f}\",\n        'p_value': f\"{t_res['p_value']:.4f}\",\n        'Significant': 'Yes' if t_res['p_value'] < 0.05 else 'No'\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-wise performance analysis\n",
    "# Note: This requires per-domain evaluation which we'll compute here\n",
    "\n",
    "def evaluate_by_domain(y_true, y_pred, domains):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for each domain.\n",
    "    \"\"\"\n",
    "    domain_metrics = []\n",
    "    \n",
    "    for domain in DOMAINS:\n",
    "        mask = domains == domain\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        y_true_domain = y_true[mask]\n",
    "        y_pred_domain = y_pred[mask]\n",
    "        \n",
    "        domain_metrics.append({\n",
    "            'Domain': domain,\n",
    "            'Samples': mask.sum(),\n",
    "            'Accuracy': accuracy_score(y_true_domain, y_pred_domain),\n",
    "            'F1_Weighted': f1_score(y_true_domain, y_pred_domain, average='weighted', zero_division=0),\n",
    "            'F1_Macro': f1_score(y_true_domain, y_pred_domain, average='macro', zero_division=0)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(domain_metrics)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOMAIN-WISE PERFORMANCE (Random Forest - Full Dataset)\")\n",
    "print(\"=\"*60)\n",
    "domain_perf_full = evaluate_by_domain(y_test_full, pred_rf_full, domains_test_full)\n",
    "print(domain_perf_full.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOMAIN-WISE PERFORMANCE (Random Forest - English-only)\")\n",
    "print(\"=\"*60)\n",
    "domain_perf_eng = evaluate_by_domain(y_test_eng, pred_rf_eng, domains_test_eng)\n",
    "print(domain_perf_eng.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics (Mean and Weighted)\n",
    "print(\"=\"*60)\n",
    "print(\"AGGREGATE PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mean performance across domains\n",
    "print(\"\\n--- Mean Performance (unweighted average across domains) ---\")\n",
    "print(f\"Full Dataset - Mean F1: {domain_perf_full['F1_Weighted'].mean():.4f}\")\n",
    "print(f\"English-only - Mean F1: {domain_perf_eng['F1_Weighted'].mean():.4f}\")\n",
    "\n",
    "# Weighted performance (weighted by number of samples)\n",
    "print(\"\\n--- Weighted Performance (weighted by domain size) ---\")\n",
    "weighted_f1_full = np.average(\n",
    "    domain_perf_full['F1_Weighted'], \n",
    "    weights=domain_perf_full['Samples']\n",
    ")\n",
    "weighted_f1_eng = np.average(\n",
    "    domain_perf_eng['F1_Weighted'], \n",
    "    weights=domain_perf_eng['Samples']\n",
    ")\n",
    "print(f\"Full Dataset - Weighted F1: {weighted_f1_full:.4f}\")\n",
    "print(f\"English-only - Weighted F1: {weighted_f1_eng:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: F1 Score comparison by classifier\n",
    "ax1 = axes[0, 0]\n",
    "classifiers = results_df['Classifier'].unique()\n",
    "x = np.arange(len(classifiers))\n",
    "width = 0.35\n",
    "\n",
    "full_f1 = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]['F1_Weighted'].values[0] \n",
    "           if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]) > 0 else 0\n",
    "           for c in classifiers]\n",
    "eng_f1 = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]['F1_Weighted'].values[0]\n",
    "          if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]) > 0 else 0\n",
    "          for c in classifiers]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, full_f1, width, label='Full (Multilingual)', color='coral')\n",
    "bars2 = ax1.bar(x + width/2, eng_f1, width, label='English-only', color='green')\n",
    "ax1.set_ylabel('F1 Score (Weighted)')\n",
    "ax1.set_title('F1 Score by Classifier and Dataset')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(classifiers)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Domain-wise F1 comparison\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(DOMAINS))\n",
    "ax2.bar(x - width/2, domain_perf_full['F1_Weighted'], width, label='Full', color='coral')\n",
    "ax2.bar(x + width/2, domain_perf_eng['F1_Weighted'], width, label='English-only', color='green')\n",
    "ax2.set_ylabel('F1 Score (Weighted)')\n",
    "ax2.set_title('Domain-wise F1 Score (Random Forest)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([d.replace('_', '\\n') for d in DOMAINS], fontsize=8)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Confusion Matrix (Full Dataset)\n",
    "ax3 = axes[1, 0]\n",
    "cm_full = confusion_matrix(y_test_full, pred_rf_full)\n",
    "sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', ax=ax3)\n",
    "ax3.set_title('Confusion Matrix - Full Dataset (RF)')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "\n",
    "# Plot 4: Confusion Matrix (English-only)\n",
    "ax4 = axes[1, 1]\n",
    "cm_eng = confusion_matrix(y_test_eng, pred_rf_eng)\n",
    "sns.heatmap(cm_eng, annot=True, fmt='d', cmap='Greens', ax=ax4)\n",
    "ax4.set_title('Confusion Matrix - English-only (RF)')\n",
    "ax4.set_xlabel('Predicted')\n",
    "ax4.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'classification_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive Model Comparison Visualization\n# Shows all models side-by-side with multiple metrics\n\nprint(\"=\"*80)\nprint(\"COMPREHENSIVE MODEL COMPARISON\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Prepare data for visualization\nmetrics_to_plot = ['F1_Weighted', 'F1_Macro', 'Balanced_Accuracy', 'Precision_Weighted', 'Recall_Weighted']\nclassifiers = results_df['Classifier'].unique()\ndatasets = ['Full (Multilingual)', 'English-only']\n\n# Color palette\ncolors_full = '#E74C3C'  # Coral/Red for Full\ncolors_eng = '#27AE60'   # Green for English-only\n\n# Plot 1: All metrics by classifier (grouped bar)\nax1 = axes[0, 0]\nx = np.arange(len(classifiers))\nwidth = 0.35\n\nf1_full = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]['F1_Weighted'].values[0] \n           if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]) > 0 else 0\n           for c in classifiers]\nf1_eng = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]['F1_Weighted'].values[0]\n          if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]) > 0 else 0\n          for c in classifiers]\n\nbars1 = ax1.bar(x - width/2, f1_full, width, label='Full Dataset', color=colors_full, alpha=0.8)\nbars2 = ax1.bar(x + width/2, f1_eng, width, label='English-only', color=colors_eng, alpha=0.8)\n\n# Add value labels on bars\nfor bar, val in zip(bars1, f1_full):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', \n             ha='center', va='bottom', fontsize=9)\nfor bar, val in zip(bars2, f1_eng):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', \n             ha='center', va='bottom', fontsize=9)\n\nax1.set_ylabel('F1 Score (Weighted)', fontsize=11)\nax1.set_title('F1 Score Comparison Across All Models', fontsize=12, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(classifiers, fontsize=10)\nax1.legend(loc='lower right')\nax1.set_ylim(0, 1.1)\nax1.grid(axis='y', alpha=0.3)\n\n# Plot 2: Balanced Accuracy comparison\nax2 = axes[0, 1]\nba_full = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]['Balanced_Accuracy'].values[0] \n           if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]) > 0 else 0\n           for c in classifiers]\nba_eng = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]['Balanced_Accuracy'].values[0]\n          if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]) > 0 else 0\n          for c in classifiers]\n\nax2.bar(x - width/2, ba_full, width, label='Full Dataset', color=colors_full, alpha=0.8)\nax2.bar(x + width/2, ba_eng, width, label='English-only', color=colors_eng, alpha=0.8)\nax2.set_ylabel('Balanced Accuracy', fontsize=11)\nax2.set_title('Balanced Accuracy Comparison', fontsize=12, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(classifiers, fontsize=10)\nax2.legend(loc='lower right')\nax2.set_ylim(0, 1.1)\nax2.grid(axis='y', alpha=0.3)\n\n# Plot 3: Performance difference (English - Full)\nax3 = axes[0, 2]\nf1_diff = [eng - full for eng, full in zip(f1_eng, f1_full)]\ncolors_diff = [colors_eng if d >= 0 else colors_full for d in f1_diff]\n\nbars = ax3.bar(classifiers, f1_diff, color=colors_diff, alpha=0.8)\nax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax3.set_ylabel('F1 Difference (English - Full)', fontsize=11)\nax3.set_title('Performance Difference\\n(Positive = English Better)', fontsize=12, fontweight='bold')\nax3.grid(axis='y', alpha=0.3)\n\n# Add value labels\nfor bar, val in zip(bars, f1_diff):\n    ypos = bar.get_height() + 0.005 if val >= 0 else bar.get_height() - 0.015\n    ax3.text(bar.get_x() + bar.get_width()/2, ypos, f'{val:+.4f}', \n             ha='center', va='bottom' if val >= 0 else 'top', fontsize=10)\n\n# Plot 4: Radar/Spider chart data preparation (multiple metrics comparison)\nax4 = axes[1, 0]\nmetrics_labels = ['F1 Weighted', 'F1 Macro', 'Bal. Acc', 'Precision', 'Recall']\nmetrics_keys = ['F1_Weighted', 'F1_Macro', 'Balanced_Accuracy', 'Precision_Weighted', 'Recall_Weighted']\n\n# Get best model performance on full vs english\nbest_full_idx = results_df[results_df['Dataset'].str.contains('Full')]['F1_Weighted'].idxmax()\nbest_eng_idx = results_df[results_df['Dataset'].str.contains('English')]['F1_Weighted'].idxmax()\nbest_full = results_df.loc[best_full_idx]\nbest_eng = results_df.loc[best_eng_idx]\n\nfull_vals = [best_full[k] for k in metrics_keys]\neng_vals = [best_eng[k] for k in metrics_keys]\n\nx_radar = np.arange(len(metrics_labels))\nax4.bar(x_radar - 0.2, full_vals, 0.4, label=f'Best Full ({best_full[\"Classifier\"]})', color=colors_full, alpha=0.8)\nax4.bar(x_radar + 0.2, eng_vals, 0.4, label=f'Best Eng ({best_eng[\"Classifier\"]})', color=colors_eng, alpha=0.8)\nax4.set_xticks(x_radar)\nax4.set_xticklabels(metrics_labels, fontsize=9)\nax4.set_ylabel('Score', fontsize=11)\nax4.set_title('Best Model Performance (All Metrics)', fontsize=12, fontweight='bold')\nax4.legend(loc='lower right')\nax4.set_ylim(0, 1.1)\nax4.grid(axis='y', alpha=0.3)\n\n# Plot 5: Training time comparison\nax5 = axes[1, 1]\ntime_data = []\ntime_labels = []\ntime_colors = []\n\nfor model in ['RF', 'SVM', 'DistilBERT']:\n    for dataset in ['Full', 'English']:\n        key = f'{model}_{dataset}'\n        if key in training_times:\n            time_data.append(training_times[key])\n            time_labels.append(f'{model}\\n({dataset[:3]})')\n            time_colors.append(colors_full if dataset == 'Full' else colors_eng)\n\nax5.bar(range(len(time_data)), time_data, color=time_colors, alpha=0.8)\nax5.set_xticks(range(len(time_data)))\nax5.set_xticklabels(time_labels, fontsize=9)\nax5.set_ylabel('Training Time (seconds)', fontsize=11)\nax5.set_title('Training Time by Model and Dataset', fontsize=12, fontweight='bold')\nax5.grid(axis='y', alpha=0.3)\n\n# Plot 6: Summary heatmap of all results\nax6 = axes[1, 2]\nheatmap_data = results_df.pivot(index='Classifier', columns='Dataset', values='F1_Weighted')\nsns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', ax=ax6, \n            cbar_kws={'label': 'F1 Score'}, vmin=0.5, vmax=1.0)\nax6.set_title('F1 Score Heatmap', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('comprehensive_model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nComprehensive visualization saved as 'comprehensive_model_comparison.png'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Conclusions\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### Dataset Analysis ###\")\n",
    "print(f\"Total samples analyzed: {len(df):,}\")\n",
    "print(f\"English samples: {df['is_english'].sum():,} ({df['is_english'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-English samples: {(~df['is_english']).sum():,} ({(~df['is_english']).mean()*100:.2f}%)\")\n",
    "print(f\"Unique languages detected: {df['detected_language'].nunique()}\")\n",
    "\n",
    "print(\"\\n### Language Distribution by Domain ###\")\n",
    "print(domain_lang_df[['Domain', 'Total', 'Non-English', 'Non-English %']].to_string(index=False))\n",
    "\n",
    "print(\"\\n### Classification Performance Summary ###\")\n",
    "print(results_df[['Classifier', 'Dataset', 'F1_Weighted', 'Balanced_Accuracy']].to_string(index=False))\n",
    "\n",
    "print(\"\\n### Hypothesis Testing Results ###\")\n",
    "print(\"H1 (Data Composition): \", end=\"\")\n",
    "non_eng_pct = (~df['is_english']).mean() * 100\n",
    "if non_eng_pct > 1:\n",
    "    print(f\"SUPPORTED - {non_eng_pct:.2f}% non-English content found\")\n",
    "else:\n",
    "    print(f\"NOT SUPPORTED - Only {non_eng_pct:.2f}% non-English content\")\n",
    "\n",
    "print(\"H2 (Performance Impact): \", end=\"\")\n",
    "# Compare best F1 scores\n",
    "if len(results_df) > 0:\n",
    "    full_best = results_df[results_df['Dataset'].str.contains('Full')]['F1_Weighted'].max()\n",
    "    eng_best = results_df[results_df['Dataset'].str.contains('English')]['F1_Weighted'].max()\n",
    "    if eng_best > full_best:\n",
    "        print(f\"SUPPORTED - English-only shows higher F1 ({eng_best:.4f} vs {full_best:.4f})\")\n",
    "    else:\n",
    "        print(f\"NOT SUPPORTED - Full dataset shows comparable/better F1 ({full_best:.4f} vs {eng_best:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "domain_lang_df.to_csv('language_distribution_by_domain.csv', index=False)\n",
    "\n",
    "# Save detailed language analysis\n",
    "df[['text', 'label', 'domain', 'detected_language', 'language_confidence', 'is_english']].to_csv(\n",
    "    'difraud_language_analysis.csv', index=False\n",
    ")\n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(\"  - classification_results.csv\")\n",
    "print(\"  - language_distribution_by_domain.csv\")\n",
    "print(\"  - difraud_language_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References and Sources\n",
    "\n",
    "### Dataset\n",
    "- **DIFrauD Dataset**: Boumber, D., et al. (2024). \"Domain-Agnostic Adapter Architecture for Deception Detection.\" LREC-COLING 2024. Available at: https://huggingface.co/datasets/difraud/difraud\n",
    "\n",
    "### Libraries and Code Sources\n",
    "- **langdetect**: Language detection library (port of Google's language-detection). https://pypi.org/project/langdetect/\n",
    "- **HuggingFace datasets**: Dataset loading library. https://huggingface.co/docs/datasets/\n",
    "- **HuggingFace transformers**: Transformer models (DistilBERT). https://huggingface.co/docs/transformers/\n",
    "- **scikit-learn**: ML classifiers (Random Forest, SVM) and metrics. https://scikit-learn.org/\n",
    "- **DistilBERT model**: distilbert-base-uncased. https://huggingface.co/distilbert-base-uncased\n",
    "\n",
    "### Academic References\n",
    "- Conneau, A., et al. (2020). \"Unsupervised cross-lingual representation learning at scale.\" ACL 2020.\n",
    "- Devlin, J., et al. (2019). \"BERT: Pre-training of deep bidirectional transformers.\" NAACL 2019.\n",
    "- Verma, R. M., et al. (2019). \"Data quality for security challenges.\" ACM CCS 2019.\n",
    "\n",
    "### Metrics Choice Justification\n",
    "- **F1-Score (Weighted)**: Used as primary metric due to class imbalance in DIFrauD dataset. Weighted F1 accounts for class distribution.\n",
    "- **F1-Score (Macro)**: Unweighted average across classes, useful for evaluating performance on minority class.\n",
    "- **Balanced Accuracy**: Accounts for class imbalance by averaging recall across classes.\n",
    "\n",
    "### Code Notes\n",
    "- All code in this notebook is original unless otherwise noted\n",
    "- API usage follows official documentation from respective libraries\n",
    "- Random seed (42) used for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}